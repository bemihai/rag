chroma:
  client:
    host: localhost                     # ChromaDB server host
    port: 8000                          # ChromaDB server port
  chunking:
    strategy: by_title                  # basic, by_title, semantic
    chunk_size: 1024
    chunk_overlap: 256
  retrieval:
    n_results: 5                        # number of chunks to retrieve per query
    similarity_threshold: 0.3           # minimum similarity score (0.0-1.0) to filter results
    deduplication_threshold: 0.9        # similarity threshold for removing duplicate chunks (optional)
    use_deduplication: false            # enable semantic deduplication of retrieved chunks
  settings:
    batch_size: 2500
    embedder: sentence-transformers/all-MiniLM-L6-v2  # embedding model for retrieval
  collections:
    - name: wine_books                  # primary collection name to query
      local_data_path: /Users/bemihai/Books/Wine
      metadata:
        description: "Professional wine books collection"
        hnsw:space: cosine              # similarity measure (cosine, l2, ip)
        hnsw:search_ef: 100             # num candidates for searching
        hnsw:construction_ef: 200       # increased for better indexing
        hnsw:num_threads: 8             # num of threads for indexing
        version: v1.1

model:
  provider: google                      # LLM provider (google, openai)
  name: gemini-2.5-flash-preview-05-20  # model name

initial_message:
  answer: "Hi there! Ask me anything about wine."